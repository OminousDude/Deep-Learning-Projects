{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>IMPORT</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dask import delayed\n",
    "from fastparquet import ParquetFile\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "import multiprocessing\n",
    "import random\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 256\n",
    "VOCAB_SIZE = 30000\n",
    "NUM_HEADS = 12\n",
    "POS_ENC_LEN = MAX_SEQ_LEN\n",
    "EMB_DIM = 768\n",
    "FEED_FORWARD_DIM = EMB_DIM * 4\n",
    "NUM_LAYERS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>PREPROCESS DATA/TOKENIZE</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERTDataset(Dataset):\n",
    "    def __init__(self, file_dict, max_seq_len, vocab_size):\n",
    "        self.path = file_dict\n",
    "\n",
    "        # Read files in chunks\n",
    "        files = glob.glob(file_dict)\n",
    "\n",
    "        ddf = dd.from_delayed([self.load_chunk(f) for f in files])\n",
    "        self.data = ddf.compute()\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer\")\n",
    "        # bert_tokenizer = self.tokenizer.train_new_from_iterator(text_iterator=self.batch_iterator(), vocab_size=VOCAB_SIZE)\n",
    "        # bert_tokenizer.save_pretrained(\"tokenizer\")\n",
    "        # self.tokenizer = bert_tokenizer\n",
    "\n",
    "        self.vocab = self.tokenizer.get_vocab()\n",
    "        self.pad_i = self.vocab['[PAD]']\n",
    "        self.mask_i = self.vocab['[MASK]']\n",
    "\n",
    "    @delayed\n",
    "    def load_chunk(self, pth):\n",
    "        x = ParquetFile(pth).to_pandas()\n",
    "        return x\n",
    "\n",
    "    def batch_iterator(self):\n",
    "        for i in tqdm(range(0, len(self.data), self.max_seq_len)):\n",
    "            yield self.data[i : i + self.max_seq_len ][\"text\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data['text'][idx]\n",
    "\n",
    "        # Encode the sentence\n",
    "        sentence = []\n",
    "        label_sentence = []\n",
    "        encoding = self.tokenizer.encode(text, max_length = self.max_seq_len, return_special_tokens_mask=True, truncation=True)\n",
    "\n",
    "        for token in encoding:\n",
    "            prob = random.random()\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "                if prob < 0.8:\n",
    "                    sentence.append(self.mask_i)\n",
    "                elif prob < 0.9:\n",
    "                    sentence.append(random.randrange(len(self.vocab)))\n",
    "                else:\n",
    "                    sentence.append(token)\n",
    "\n",
    "                label_sentence.append(token)\n",
    "            else:\n",
    "                sentence.append(token)\n",
    "                label_sentence.append(0)\n",
    "\n",
    "        padding = [self.pad_i for _ in range(self.max_seq_len - len(sentence))]\n",
    "        sentence.extend(padding)\n",
    "        label_sentence.extend(padding)\n",
    "\n",
    "        sentence = torch.as_tensor(sentence)\n",
    "        label_sentence = torch.as_tensor(label_sentence)\n",
    "\n",
    "        return sentence, label_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MERTDataset('/home/maxim/Downloads/MERT-DATA/', max_seq_len=MAX_SEQ_LEN, vocab_size=VOCAB_SIZE)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>EMBEDDINGS</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model, device='cuda').float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "       \n",
    "    def forward(self, sequence):\n",
    "        x = self.token(sequence) + self.position(sequence)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>ATTENTION</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()  \n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)\n",
    "\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, -1e4)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        context = torch.matmul(weights, value)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "        return self.output_linear(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>OPTIMIZER</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self, scalar):\n",
    "        self.update_learning_rate()\n",
    "        scalar.step(self.optimizer)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self.get_lr_scale()\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>MODEL</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, middle_dim=FEED_FORWARD_DIM, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=EMB_DIM, heads=NUM_HEADS, feed_forward_hidden=FEED_FORWARD_DIM, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERT(nn.Module):\n",
    "    def __init__(self, vocab_size = VOCAB_SIZE, d_model=EMB_DIM, n_layers=NUM_LAYERS, heads=NUM_HEADS, feed_forward_dim=FEED_FORWARD_DIM, seq_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        self.feed_forward_hidden = feed_forward_dim\n",
    "        \n",
    "        self.embedding = MERTEmbedding(vocab_size=vocab_size, embed_size=d_model, seq_len=seq_len)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, heads, feed_forward_dim, dropout) for _ in range(n_layers)])\n",
    "        \n",
    "    def save(self, step, epoch = None, optim = None):\n",
    "        now = datetime.now()\n",
    "\n",
    "        dt_string = now.strftime(\"%d|%m|%Y %H:%M:%S\")\n",
    "        \n",
    "        if epoch == None:\n",
    "            torch.save(self, \"saves/MERT_time: \" + dt_string + \"|step: \" + step.__str__()  + \".pt\")\n",
    "        else:\n",
    "            torch.save(self, \"saves/MERT_time: \" + dt_string + \"|epoch: \" + epoch.__str__()  + \".pt\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x>0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1).to('cuda')\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder.forward(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>Language Model</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(nn.Module):\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "\n",
    "class MERTLM(nn.Module):\n",
    "    def __init__(self, bert: MERT, emb_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.mask_lm = MaskedLanguageModel(emb_dim, vocab_size)\n",
    "\n",
    "    def save(self, step, epoch = None, optim = None):\n",
    "        self.bert.save(step, epoch, optim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x)\n",
    "        return self.mask_lm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>TRAINING</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERTTrainer:#2e-4\n",
    "    def __init__(self, model, loader, lr = 1e-4, weight_decay=0.01,\n",
    "                betas=(0.9, 0.999), warmup_steps=10000, log_freq=10, device='cuda'):\n",
    "        self.device = device\n",
    "        self.model = torch.compile(model.to('cuda'))\n",
    "        self.loader = loader\n",
    "\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        self.optim = torch.optim.Adam(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(self.optim, EMB_DIM, n_warmup_steps=warmup_steps)\n",
    "\n",
    "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
    "        self.log_freq = log_freq\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.loader)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.loader, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        loss_sum_epoch = 0\n",
    "        loss_sum_steps = 0\n",
    "        step = 0\n",
    "        step_no_reset = 0\n",
    "        loss_graph = []\n",
    "\n",
    "        cola = 0\n",
    "\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        # data_iter = tqdm.tqdm(\n",
    "        #     enumerate(data_loader),\n",
    "        #     desc=\"EP_%s:%d\" % (mode, epoch),\n",
    "        #     total=len(data_loader),\n",
    "        #     bar_format=\"{l_bar}{r_bar}\"\n",
    "        # )\n",
    "\n",
    "        for data, labels in iter(data_loader):\n",
    "            # if i + BATCH_SIZE > df.shape[0]:\n",
    "            #     break\n",
    "\n",
    "            data = data.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # data = torch.stack(data).cuda()\n",
    "            # labels = torch.stack(labels).cuda()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                mask_lm_output = self.model.forward(data)\n",
    "\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), labels)\n",
    "            loss = mask_loss\n",
    "            loss_sum_steps += loss\n",
    "            loss_sum_epoch += loss\n",
    "\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.optim_schedule.step_and_update_lr(self.scaler)\n",
    "\n",
    "            if step_no_reset % 10000 == 0:\n",
    "                temp = pd.DataFrame()\n",
    "                temp['data'] = np.array(torch.as_tensor(loss_graph, device='cpu'))\n",
    "                moving_average = temp['data'].rolling(window=5).mean()\n",
    "                moving_average_large = temp['data'].rolling(window=15).mean()\n",
    "                plt.plot(torch.as_tensor(loss_graph, device='cpu'), label=\"Original Data\", color='black')\n",
    "                plt.plot(torch.as_tensor(moving_average, device='cpu'), label=\"Scaled Data\", color='pink')\n",
    "                plt.plot(torch.as_tensor(moving_average_large, device='cpu'), label=\"Scaled Data_10\", color='blue')\n",
    "                plt.ylabel(\"loss\")\n",
    "                plt.savefig(\"saves/loss_\" + str(step_no_reset) + \".png\")\n",
    "\n",
    "            if step_no_reset % 10000 == 0:\n",
    "                self.model.save(step_no_reset)\n",
    "            \n",
    "            if step % 1000 == 0:\n",
    "                print(loss_sum_steps / step)\n",
    "                loss_sum_steps = 0\n",
    "                step = 0\n",
    "                loss_graph.append(loss)\n",
    "            step += 1\n",
    "            step_no_reset += 1\n",
    "\n",
    "            self.scaler.update()\n",
    "\n",
    "        print(\n",
    "            f\"EP{epoch}, AVG LOSS{loss_sum_epoch / i}\"\n",
    "        )\n",
    "        self.model.save(0, epoch)\n",
    "\n",
    "        plt.plot(loss_graph)\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.savefig(\"saves/loss_\" + str(step_no_reset) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 131146032\n",
      "tensor(inf, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(6.7610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(5.1958, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(5.0951, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(5.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(5.0264, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(5.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(5.0959, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(5.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(5.0049, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.9511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.9537, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MERT(VOCAB_SIZE, EMB_DIM, NUM_LAYERS, NUM_HEADS, FEED_FORWARD_DIM, MAX_SEQ_LEN, 0.1)\n",
    "# model = torch.load('saves/MERT_time: 16|03|2024 14:51:54|epoch: 1.pt')\n",
    "#18 sec\n",
    "LM = MERTLM(model, EMB_DIM, VOCAB_SIZE)\n",
    "trainer = MERTTrainer(LM, loader=loader)\n",
    "trainer.train(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>TESTING</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text\n",
      "0     Our friends won't buy this analysis, let alone...\n",
      "1     One more pseudo generalization and I'm giving up.\n",
      "2      One more pseudo generalization or I'm giving up.\n",
      "3        The more we study verbs, the crazier they get.\n",
      "4             Day by day the facts are getting murkier.\n",
      "...                                                 ...\n",
      "8546                   Poseidon appears to own a dragon\n",
      "8547                     Digitize is my happiest memory\n",
      "8548                     It is easy to slay the Gorgon.\n",
      "8549       I had the strangest feeling that I knew you.\n",
      "8550                What all did you get for Christmas?\n",
      "\n",
      "[8551 rows x 1 columns]\n",
      "tensor(7.1513, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(inf, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No inf checks were recorded prior to update.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m trainer\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtest_data \u001b[38;5;241m=\u001b[39m ddf\n\u001b[0;32m---> 15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m glue_metric \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcola\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m glue_score \u001b[38;5;241m=\u001b[39m glue_metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mglue_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[22], line 22\u001b[0m, in \u001b[0;36mMERTTrainer.test\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 122\u001b[0m, in \u001b[0;36mMERTTrainer.iteration\u001b[0;34m(self, epoch, data_loader, train)\u001b[0m\n\u001b[1;32m    119\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    120\u001b[0m     step_no_reset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEP\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AVG LOSS\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_sum_epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;241m0\u001b[39m, epoch)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:508\u001b[0m, in \u001b[0;36mGradScaler.update\u001b[0;34m(self, new_scale)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# Consume shared inf/nan data collected from optimizers to update the scale.\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# If all found_inf tensors are on the same device as self._scale, this operation is asynchronous.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     found_infs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    503\u001b[0m         found_inf\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_per_optimizer_states\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m found_inf \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    506\u001b[0m     ]\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(found_infs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded prior to update.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m     found_inf_combined \u001b[38;5;241m=\u001b[39m found_infs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(found_infs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: No inf checks were recorded prior to update."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "glue_dataset = datasets.load_dataset(\"glue\", \"cola\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['text'] = glue_dataset['train']['sentence']\n",
    "ddf = dd.from_pandas(df, npartitions=1).compute()\n",
    "\n",
    "print(ddf)\n",
    "\n",
    "model = trainer\n",
    "model.test_data = ddf\n",
    "\n",
    "predictions = model.test(1)\n",
    "\n",
    "glue_metric = datasets.load_metric(\"glue\", \"cola\")\n",
    "glue_score = glue_metric.compute(predictions=predictions, references=glue_dataset[\"labels\"])\n",
    "\n",
    "print(glue_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
