{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>IMPORT</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/anaconda3/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:17: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 12.0.1. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dask import delayed\n",
    "from fastparquet import ParquetFile\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "import multiprocessing\n",
    "import random\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 256\n",
    "VOCAB_SIZE = 30000\n",
    "NUM_HEADS = 12\n",
    "POS_ENC_LEN = MAX_SEQ_LEN\n",
    "EMB_DIM = 768\n",
    "FEED_FORWARD_DIM = EMB_DIM * 4\n",
    "NUM_LAYERS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>PREPROCESS DATA/TOKENIZE</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERTDataset(Dataset):\n",
    "    def __init__(self, file_dict, max_seq_len, vocab_size):\n",
    "        self.path = file_dict\n",
    "\n",
    "        # Read files in chunks\n",
    "        files = glob.glob(file_dict)\n",
    "\n",
    "        ddf = dd.from_delayed([self.load_chunk(f) for f in files])\n",
    "        self.data = ddf.compute()\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer\")\n",
    "        bert_tokenizer = self.tokenizer.train_new_from_iterator(text_iterator=self.batch_iterator(), vocab_size=VOCAB_SIZE)\n",
    "        bert_tokenizer.save_pretrained(\"tokenizer\")\n",
    "        self.tokenizer = bert_tokenizer\n",
    "\n",
    "        self.vocab = self.tokenizer.get_vocab()\n",
    "        self.pad_i = self.vocab['[PAD]']\n",
    "        self.mask_i = self.vocab['[MASK]']\n",
    "\n",
    "    @delayed\n",
    "    def load_chunk(self, pth):\n",
    "        x = ParquetFile(pth).to_pandas()\n",
    "        return x\n",
    "\n",
    "    def batch_iterator(self):\n",
    "        for i in tqdm(range(0, len(self.data), self.max_seq_len)):\n",
    "            yield self.data[i : i + self.max_seq_len ][\"text\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data['text'][idx]\n",
    "\n",
    "        # Encode the sentence\n",
    "        sentence = []\n",
    "        label_sentence = []\n",
    "        encoding = self.tokenizer.encode(text, max_length = self.max_seq_len, return_special_tokens_mask=True, truncation=True)\n",
    "\n",
    "        for token in encoding:\n",
    "            prob = random.random()\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "                if prob < 0.8:\n",
    "                    sentence.append(self.mask_i)\n",
    "                elif prob < 0.9:\n",
    "                    sentence.append(random.randrange(len(self.vocab)))\n",
    "                else:\n",
    "                    sentence.append(token)\n",
    "\n",
    "                label_sentence.append(token)\n",
    "            else:\n",
    "                sentence.append(token)\n",
    "                label_sentence.append(0)\n",
    "\n",
    "        padding = [self.pad_i for _ in range(self.max_seq_len - len(sentence))]\n",
    "        sentence.extend(padding)\n",
    "        label_sentence.extend(padding)\n",
    "\n",
    "        sentence = torch.as_tensor(sentence)\n",
    "        label_sentence = torch.as_tensor(label_sentence)\n",
    "\n",
    "        return sentence, label_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30485/30485 [00:59<00:00, 514.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "BertTokenizerFast(name_or_path='tokenizer', vocab_size=30000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "dataset = MERTDataset('/media/maxim/DataSets/MERT/MERT-DATA/', vocab_size=VOCAB_SIZE, max_seq_len=MAX_SEQ_LEN)\n",
    "tokenizer = dataset.tokenizer\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(tokenizer)\n",
    "\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [MASK] ` welcome, my dear [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next(iter(loader))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 112,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader))[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>EMBEDDINGS</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model, device='cuda').float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "       \n",
    "    def forward(self, sequence):\n",
    "        x = self.token(sequence) + self.position(sequence)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>ATTENTION</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()  \n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)\n",
    "\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, -1e4)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        context = torch.matmul(weights, value)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "        return self.output_linear(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>OPTIMIZER</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self, scalar):\n",
    "        self.update_learning_rate()\n",
    "        scalar.step(self.optimizer)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self.get_lr_scale()\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>MODEL</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, middle_dim=FEED_FORWARD_DIM, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=EMB_DIM, heads=NUM_HEADS, feed_forward_hidden=FEED_FORWARD_DIM, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERT(nn.Module):\n",
    "    def __init__(self, vocab_size = VOCAB_SIZE, d_model=EMB_DIM, n_layers=NUM_LAYERS, heads=NUM_HEADS, feed_forward_dim=FEED_FORWARD_DIM, seq_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        self.feed_forward_hidden = feed_forward_dim\n",
    "        \n",
    "        self.embedding = MERTEmbedding(vocab_size=vocab_size, embed_size=d_model, seq_len=seq_len)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, heads, feed_forward_dim, dropout) for _ in range(n_layers)])\n",
    "        \n",
    "    def save(self, step, epoch = None, optim = None):\n",
    "        now = datetime.now()\n",
    "\n",
    "        dt_string = now.strftime(\"%d|%m|%Y %H:%M:%S\")\n",
    "        \n",
    "        if epoch == None:\n",
    "            torch.save(self, \"saves/MERT_time: \" + dt_string + \"|step: \" + step.__str__()  + \".pt\")\n",
    "        else:\n",
    "            torch.save(self, \"saves/MERT_time: \" + dt_string + \"|epoch: \" + epoch.__str__()  + \".pt\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x>0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1).to('cuda')\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder.forward(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>Language Model</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(nn.Module):\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = self.linear(x)\n",
    "        out = self.softmax(x)\n",
    "        return out\n",
    "\n",
    "class MERTLM(nn.Module):\n",
    "    def __init__(self, bert: MERT, emb_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.mask_lm = MaskedLanguageModel(emb_dim, vocab_size)\n",
    "\n",
    "    def save(self, step, epoch = None, optim = None):\n",
    "        self.bert.save(step, epoch, optim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x)\n",
    "        return self.mask_lm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>TRAINING</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERTTrainer:\n",
    "    def __init__(self, model, vocab, lr = 3e-5, weight_decay=0.01,\n",
    "                betas=(0.9, 0.999), warmup_steps=10000, log_freq=10, device='cuda'):\n",
    "        self.device = device\n",
    "        self.model = torch.compile(model.to('cuda'))\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        self.optim = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "\n",
    "        # self.optim = torch.optim.Adam(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        # self.optim_schedule = ScheduledOptim(self.optim, EMB_DIM, n_warmup_steps=warmup_steps)\n",
    "\n",
    "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "        self.log_freq = log_freq\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "    \n",
    "    def train(self, loader, epoch):\n",
    "        self.iteration(epoch, loader)\n",
    "\n",
    "    def test(self, loader, epoch, tokenizer):\n",
    "        self.iteration(epoch, loader, tokenizer=tokenizer, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, tokenizer=None, train=True):\n",
    "        loss_sum_epoch = 0\n",
    "        loss_sum_steps = 0\n",
    "        step = 0\n",
    "        step_no_reset = 0\n",
    "        loss_graph = []\n",
    "        cola = 0\n",
    "\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        # data_iter = tqdm.tqdm(\n",
    "        #     enumerate(data_loader),\n",
    "        #     desc=\"EP_%s:%d\" % (mode, epoch),\n",
    "        #     total=len(data_loader),\n",
    "        #     bar_format=\"{l_bar}{r_bar}\"\n",
    "        # )\n",
    "\n",
    "        # Iterate through dataloader\n",
    "        for data, labels in tqdm(data_loader):\n",
    "            # if i + BATCH_SIZE > df.shape[0]:\n",
    "            #     break\n",
    "\n",
    "            data = data.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # data = torch.stack(data).cuda()\n",
    "            # labels = torch.stack(labels).cuda()\n",
    "\n",
    "            # Convert data to 16 bit\n",
    "            with torch.cuda.amp.autocast():\n",
    "                mask_lm_output = self.model.forward(data)\n",
    "                \n",
    "            # Perform cross entropy loss\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), labels)\n",
    "            loss = mask_loss\n",
    "            loss_sum_steps += loss\n",
    "            loss_sum_epoch += loss\n",
    "\n",
    "            # Train the model and normalize (scale) the data scaler\n",
    "            if train:\n",
    "                self.optim.zero_grad()\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optim)\n",
    "                self.scaler.update()\n",
    "\n",
    "            # Create loss img and save the model\n",
    "            if step_no_reset % 1000 == 0:\n",
    "                temp = pd.DataFrame()\n",
    "                temp['data'] = np.array(torch.as_tensor(loss_graph, device='cpu'))\n",
    "                moving_average = temp['data'].rolling(window=5).mean()\n",
    "                moving_average_large = temp['data'].rolling(window=20).mean()\n",
    "                plt.plot(torch.as_tensor(loss_graph, device='cpu'), label=\"Original Data\", color='black')\n",
    "                plt.plot(torch.as_tensor(moving_average, device='cpu'), label=\"Scaled Data\", color='pink')\n",
    "                plt.plot(torch.as_tensor(moving_average_large, device='cpu'), label=\"Scaled Data_10\", color='blue')\n",
    "                plt.ylabel(\"loss\")\n",
    "                plt.savefig(\"saves/loss_\" + str(step_no_reset) + \".png\")\n",
    "                \n",
    "                self.model.save(step_no_reset)\n",
    "\n",
    "                data_list = []\n",
    "                for val in data[0]:\n",
    "                    data_list.append(self.vocab[val.item()])\n",
    "                print(\"Original Data: \" + str(data_list))\n",
    "                \n",
    "                pred_list = []\n",
    "                for vals in mask_lm_output[0]:\n",
    "                    pred_list.append(self.vocab[vals.argmax().item()])\n",
    "                print(\"Predicted Data: \" + str(pred_list))\n",
    "\n",
    "                # mask_list = []\n",
    "                # for val in labels[0]:\n",
    "                #     mask_list.append(self.vocab[val.item()])\n",
    "                # print(\"Mask: \" + str(mask_list))\n",
    "            \n",
    "            # Save a loss checkpoint\n",
    "            if step % 500 == 0:\n",
    "                loss_sum_steps = 0\n",
    "                step = 0\n",
    "                loss_graph.append(loss)\n",
    "                print(loss)\n",
    "            step += 1\n",
    "            step_no_reset += 1\n",
    "\n",
    "        print(\n",
    "            f\"EP{epoch}, AVG LOSS{loss_sum_epoch / step_no_reset}\"\n",
    "        )\n",
    "        self.model.save(0, epoch)\n",
    "\n",
    "        temp = pd.DataFrame()\n",
    "        temp['data'] = np.array(torch.as_tensor(loss_graph, device='cpu'))\n",
    "        moving_average = temp['data'].rolling(window=5).mean()\n",
    "        moving_average_large = temp['data'].rolling(window=20).mean()\n",
    "        plt.plot(torch.as_tensor(loss_graph, device='cpu'), label=\"Original Data\", color='black')\n",
    "        plt.plot(torch.as_tensor(moving_average, device='cpu'), label=\"Scaled Data\", color='pink')\n",
    "        plt.plot(torch.as_tensor(moving_average_large, device='cpu'), label=\"Scaled Data_10\", color='blue')\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.savefig(\"saves/loss_\" + str(step_no_reset) + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor(6.7584, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "tensor(5.1880, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "tensor(5.0820, device='cuda:0', grad_fn=<DivBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict((v,k) for k,v in tokenizer.get_vocab().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 131146032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/121938 [00:19<658:32:35, 19.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: ['[CLS]', 'i', '[MASK]', '[MASK]', 'steering', 'and', 'hit', '[MASK]', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Predicted Data: ['resp', 'histor', 'paraded', 'resp', 'resp', 'dish', 'resp', 'paraded', 'resp', 'chiff', 'stake', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'weighs', '##tin', 'thrall', 'resp', 'resp', 'encompassing', 'resp', 'resp', 'resp', 'resp', 'resp', 'foot', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', '##mouthed', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'foot', 'resp', 'resp', 'sugar', 'resp', 'resp', 'colorful', 'resp', 'resp', '##ened', 'aval', 'resp', 'resp', 'resp', 'sugar', 'nip', 'resp', 'resp', 'resp', 'resp', 'resp', 'rob', 'nip', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'vlad', 'resp', 'resp', 'longest', 'resp', 'consequence', 'resp', 'basket', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'pharmaceutical', 'resp', 'resp', 'ph', 'resp', 'raggedly', 'germs', 'resp', 'resp', '##ooo', 'resp', 'resp', 'resp', 'resp', 'resp', 'mythical', 'raggedly', 'enveloping', 'resp', 'glorious', 'resp', 'radio', 'tatiana', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'rob', 'resp', 'resp', '##rious', 'resp', 'resp', 'trev', 'resp', 'resp', 'resp', 'resp', 'resp', '##uccess', '##rious', 'resp', 'resp', '##ombs', 'resp', 'resp', 'resp', 'resp', 'nikolas', 'resp', 'ruckus', 'shes', '##ations', 'resp', 'mul', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'kristen', 'resp', 'resp', 'resp', 'resp', 'resp', 'consequence', 'basket', 'resp', 'crumpled', 'resp', 'resp', 'tatiana', 'pitiful', 'resp', 'basket', 'resp', 'resp', 'resp', 'resp', 'resp', '##00', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'housegu', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'resp', 'nemours', 'resp', 'resp', 'nip', 'resp', 'resp', 'resp', 'basket', 'resp', '##00', 'tatiana', 'resp']\n",
      "tensor(10.5993, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 502/121938 [01:44<6:01:12,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0717, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1002/121938 [03:14<8:07:06,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: ['[CLS]', '`', '`', 'have', 'a', 'little', 'faith', ',', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Predicted Data: ['[CLS]', '`', '`', 'have', 'a', \"'\", \"'\", ',', ',', '.', '[SEP]', \"'\", \"'\", '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "tensor(4.9517, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1502/121938 [04:43<5:54:55,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5597, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1525/121938 [04:47<5:55:19,  5.65it/s]"
     ]
    }
   ],
   "source": [
    "model = MERT(VOCAB_SIZE, EMB_DIM, NUM_LAYERS, NUM_HEADS, FEED_FORWARD_DIM, MAX_SEQ_LEN, 0.1)\n",
    "# model = torch.load('saves/MERT_time: 16|03|2024 14:51:54|epoch: 1.pt')\n",
    "#18 sec\n",
    "LM = MERTLM(model, EMB_DIM, VOCAB_SIZE)\n",
    "trainer = MERTTrainer(LM, vocab=vocab)\n",
    "trainer.train(loader, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>TESTING</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My horse is fat!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  156, 3066,  228, 4231,    5,    3]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = torch.zeros(MAX_SEQ_LEN - tokens[0].shape[0])\n",
    "\n",
    "tokens = torch.cat((tokens[0], pad), 0).unsqueeze(0).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = LM.forward(tokens.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.6564e+01, -1.7534e+01, -2.8635e-03,  ..., -1.5963e+01,\n",
      "          -1.6226e+01, -1.7449e+01],\n",
      "         [-1.3256e+01, -1.4082e+01, -9.2687e+00,  ..., -1.2687e+01,\n",
      "          -1.2810e+01, -1.3123e+01],\n",
      "         [-1.2635e+01, -1.3247e+01, -9.5519e+00,  ..., -1.3078e+01,\n",
      "          -1.2382e+01, -1.3091e+01],\n",
      "         ...,\n",
      "         [-1.2703e+01, -1.3454e+01, -8.2831e+00,  ..., -1.2609e+01,\n",
      "          -1.1184e+01, -1.2762e+01],\n",
      "         [-1.3142e+01, -1.3860e+01, -9.1748e+00,  ..., -1.2968e+01,\n",
      "          -1.1605e+01, -1.3352e+01],\n",
      "         [-1.3251e+01, -1.4069e+01, -9.3818e+00,  ..., -1.3210e+01,\n",
      "          -1.1879e+01, -1.3550e+01]]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] my horse is fat ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n"
     ]
    }
   ],
   "source": [
    "pred_str = \"\"\n",
    "\n",
    "for vals in tokens[0]:\n",
    "    pred_str += vocab[vals.item()] + \" \"\n",
    "\n",
    "print(pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] my he . . . [SEP] ' . [SEP] . [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] . [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] \n"
     ]
    }
   ],
   "source": [
    "vocab = dict((v,k) for k,v in tokenizer.get_vocab().items())\n",
    "pred_str = \"\"\n",
    "for vals in preds[0]:\n",
    "    pred_str += vocab[vals.argmax().item()] + \" \"\n",
    "\n",
    "print(pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "glue_dataset = datasets.load_dataset(\"glue\", \"cola\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['text'] = glue_dataset['train']['sentence']\n",
    "ddf = dd.from_pandas(df, npartitions=1).compute()\n",
    "\n",
    "print(ddf)\n",
    "\n",
    "model = trainer\n",
    "model.test_data = ddf\n",
    "\n",
    "predictions = model.test(1)\n",
    "\n",
    "glue_metric = datasets.load_metric(\"glue\", \"cola\")\n",
    "glue_score = glue_metric.compute(predictions=predictions, references=glue_dataset[\"labels\"])\n",
    "\n",
    "print(glue_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
