{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/anaconda3/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:17: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 12.0.1. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM, Trainer, TrainingArguments, BertTokenizerFast\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dask import delayed\n",
    "from fastparquet import ParquetFile\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast, RobertaTokenizerFast, AutoTokenizer\n",
    "import multiprocessing\n",
    "import random\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 256\n",
    "VOCAB_SIZE = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERTDataset(Dataset):\n",
    "    def __init__(self, file_dict, max_seq_len, vocab_size, dataframe=None):\n",
    "        self.path = file_dict\n",
    "\n",
    "        if file_dict:\n",
    "            # Read files in chunks\n",
    "            files = glob.glob(file_dict)\n",
    "            print(files.__len__())\n",
    "\n",
    "            ddf = dd.from_delayed([self.load_chunk(f) for f in files])\n",
    "            self.data = ddf.compute()\n",
    "        else:\n",
    "            self.data = dataframe\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")#RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "        # bert_tokenizer = self.tokenizer.train_new_from_iterator(text_iterator=self.batch_iterator(), vocab_size=VOCAB_SIZE)\n",
    "        # bert_tokenizer.save_pretrained(\"tokenizer\")\n",
    "        # self.tokenizer = bert_tokenizer\n",
    "\n",
    "        self.vocab = self.tokenizer.get_vocab()\n",
    "        self.pad_i = self.vocab['[PAD]']\n",
    "        self.mask_i = self.vocab['[MASK]']\n",
    "\n",
    "    @delayed\n",
    "    def load_chunk(self, pth):\n",
    "        x = ParquetFile(pth).to_pandas()\n",
    "        return x\n",
    "\n",
    "    def batch_iterator(self):\n",
    "        for i in tqdm(range(0, len(self.data), self.max_seq_len)):\n",
    "            yield self.data[i : i + self.max_seq_len ][\"text\"]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data['text'][idx]\n",
    "\n",
    "        # Encode the sentence\n",
    "        sentence = []\n",
    "        label_sentence = []\n",
    "        encoding = self.tokenizer.encode(text, max_length = self.max_seq_len, return_special_tokens_mask=True, truncation=True)\n",
    "        attention_mask = [0] * self.max_seq_len\n",
    "\n",
    "        i = 0\n",
    "        for token in encoding:\n",
    "            attention_mask[i] = 1\n",
    "            prob = random.random()\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "                if prob < 0.8:\n",
    "                    sentence.append(self.mask_i)\n",
    "                elif prob < 0.9:\n",
    "                    sentence.append(random.randrange(len(self.vocab)))\n",
    "                else:\n",
    "                    sentence.append(token)\n",
    "\n",
    "                label_sentence.append(token)\n",
    "            else:\n",
    "                sentence.append(token)\n",
    "                label_sentence.append(-100)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        # Pad sentence so that all sentences have same length\n",
    "        padding = [self.pad_i for _ in range(self.max_seq_len - len(sentence))]\n",
    "        sentence.extend(padding)\n",
    "        padding = [-100 for _ in range(self.max_seq_len - len(label_sentence))]\n",
    "        label_sentence.extend(padding)\n",
    "\n",
    "        sentence = torch.as_tensor(sentence)\n",
    "        label_sentence = torch.as_tensor(label_sentence)\n",
    "        attention_mask = torch.as_tensor(attention_mask)\n",
    "\n",
    "        return {'input_ids': sentence, 'labels': label_sentence, 'attention_mask': attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "dataset = MERTDataset('/media/maxim/DataSets/MERT/MERT-DATA/', max_seq_len=MAX_SEQ_LEN, vocab_size=VOCAB_SIZE)\n",
    "tokenizer = dataset.tokenizer\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712625758.852424   10451 cpu_client.cc:370] TfrtCpuClient created.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaxim-g\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/maxim/Documents/MyProjects/DeepLearning/MERT/wandb/run-20240408_212240-8pi8mbtw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maxim-g/huggingface/runs/8pi8mbtw' target=\"_blank\">winter-pine-42</a></strong> to <a href='https://wandb.ai/maxim-g/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maxim-g/huggingface' target=\"_blank\">https://wandb.ai/maxim-g/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maxim-g/huggingface/runs/8pi8mbtw' target=\"_blank\">https://wandb.ai/maxim-g/huggingface/runs/8pi8mbtw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87765f0c882f47539d513dcfaaa3429b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model = BertForMaskedLM.from_pretrained('bert-base-uncased')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, args\u001b[38;5;241m=\u001b[39mtrain_args, train_dataset\u001b[38;5;241m=\u001b[39mdataset)\n\u001b[0;32m----> 9\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1911\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1906\u001b[0m         nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   1907\u001b[0m             amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[1;32m   1908\u001b[0m             args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   1909\u001b[0m         )\n\u001b[1;32m   1910\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1911\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   1912\u001b[0m             model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m   1913\u001b[0m             args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   1914\u001b[0m         )\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:2100\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_gradients()\n\u001b[0;32m-> 2100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_xla/_patched_functions.py:49\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m (total_norm\u001b[38;5;241m.\u001b[39misnan() \u001b[38;5;129;01mor\u001b[39;00m total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m     45\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     46\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for a gradient from `parameters` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     47\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis non-finite, so it cannot be clipped. This error can be \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisabled with `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m clip_coef \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(max_norm, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;241m/\u001b[39m (total_norm \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m     50\u001b[0m clip_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(clip_coef \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m, clip_coef,\n\u001b[1;32m     51\u001b[0m                          torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_args = TrainingArguments(output_dir=\"output_dir\", save_steps=10000, bf16=True, torch_compile=True, per_device_train_batch_size=64)\n",
    "\n",
    "config = BertConfig()\n",
    "\n",
    "model = BertForMaskedLM(config=config)\n",
    "# model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "trainer = Trainer(model, args=train_args, train_dataset=dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('output_dir/checkpoint-10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1996, 3586, 2003,  103, 6638, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "text = 'The horse is [MASK] fat.'\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "vocab_swap = dict((v,k) for k,v in tokenizer.get_vocab().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = torch.zeros(MAX_SEQ_LEN - tokens['input_ids'][0].shape[0])\n",
    "\n",
    "input_ids = torch.cat((tokens['input_ids'][0], pad), 0).unsqueeze(0).int()\n",
    "attention_mask = torch.cat((tokens['attention_mask'][0], pad), 0).unsqueeze(0).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] the horse is [MASK] fat . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n"
     ]
    }
   ],
   "source": [
    "pred_str = \"\"\n",
    "\n",
    "for vals in input_ids[0]:\n",
    "    pred_str += vocab_swap[vals.item()] + \" \"\n",
    "\n",
    "print(pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1996, 3586, 2003,  103, 6638, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1996, 3586, 2003,  103, 6638, 1012,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]], dtype=torch.int32), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedLMOutput(loss=None, logits=tensor([[[ -5.3164,  -3.5232,  -3.5342,  ...,  -2.8094,  -3.6205,  -3.5154],\n",
      "         [-10.5610,  -6.7457,  -5.4357,  ...,  -6.0925,  -6.6187,  -9.2391],\n",
      "         [ -7.8776,  -3.8386,  -3.8514,  ...,  -3.9736,  -4.6557,  -6.3922],\n",
      "         ...,\n",
      "         [ -8.8214,  -5.8837,  -4.2537,  ...,  -5.3946,  -5.6697,  -7.1925],\n",
      "         [ -9.1171,  -6.3222,  -4.4915,  ...,  -5.6766,  -5.9392,  -7.3581],\n",
      "         [ -9.7064,  -7.4902,  -5.4273,  ...,  -6.2417,  -6.4642,  -7.9772]]],\n",
      "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] the horse is a fat . [SEP] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . there . . . . . . . . . there . . . it there . . . . it there there . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . [SEP] \n"
     ]
    }
   ],
   "source": [
    "vocab = dict((v,k) for k,v in tokenizer.get_vocab().items())\n",
    "pred_str = \"\"\n",
    "for vals in outputs[0][0]:\n",
    "    pred_str += vocab_swap[vals.argmax().item()] + \" \"\n",
    "\n",
    "print(pred_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
