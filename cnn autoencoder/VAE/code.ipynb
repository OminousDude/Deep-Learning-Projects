{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.strategies import DeepSpeedStrategy\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage):\n",
    "        mnist_full = train_dataset = datasets.MNIST(\n",
    "            root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",
    "        )\n",
    "        self.mnist_test = datasets.MNIST(\n",
    "            root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n",
    "        )\n",
    "        self.mnist_train, self.mnist_val = torch.utils.data.random_split(\n",
    "            mnist_full, [55000, 5000]\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "module = MNISTDataModule(32, 20)\n",
    "module.setup(\"fit\")\n",
    "print(len(module.mnist_train))\n",
    "print(len(module.mnist_val))\n",
    "print(len(module.mnist_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 784])\n"
     ]
    }
   ],
   "source": [
    "class VAEpl(pl.LightningModule):\n",
    "    def __init__(self, lr, input_dim=784, h_dim=200, z_dim=20):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.loss_fn = nn.BCELoss(reduction=\"sum\")\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.img_2hid = nn.Linear(input_dim, h_dim)\n",
    "        self.hid_2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.hid_2sigma = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "        self.z_2hid = nn.Linear(z_dim, h_dim)\n",
    "        self.hid_2img = nn.Linear(h_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.relu(self.img_2hid(x))\n",
    "        mu, sigma = self.hid_2mu(h), self.hid_2sigma(h)\n",
    "        return mu, sigma\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.relu(self.z_2hid(z))\n",
    "        return torch.sigmoid(self.hid_2img(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encode(x)\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z_new = mu + sigma * epsilon\n",
    "        x_reconstructed = self.decode(z_new)\n",
    "        return x_reconstructed, mu, sigma\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x_reconstructed, mu, sigma = self.forward(x)\n",
    "        reconstruction_loss = self.loss_fn(x_reconstructed, x)\n",
    "        kl_div = -torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "        loss = reconstruction_loss + kl_div\n",
    "        self.log(\"train_loss\", loss, sync_dist=True)\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            x = x[:8]\n",
    "            x_reconstructed = x_reconstructed[:8]\n",
    "            grid = torchvision.utils.make_grid(x_reconstructed.view(-1, 1, 28, 28))\n",
    "            self.logger.experiment.add_image(\"reconstructed\", grid, self.global_step)\n",
    "            grid = torchvision.utils.make_grid(x.view(-1, 1, 28, 28))\n",
    "            self.logger.experiment.add_image(\"original\", grid, self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x_reconstructed, mu, sigma = self.forward(x)\n",
    "        reconstruction_loss = self.loss_fn(x_reconstructed, x)\n",
    "        kl_div = -torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "        loss = reconstruction_loss + kl_div\n",
    "        self.log(\"val_loss\", loss, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x_reconstructed, mu, sigma = self.forward(x)\n",
    "        reconstruction_loss = self.loss_fn(x_reconstructed, x)\n",
    "        kl_div = -torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "        loss = reconstruction_loss + kl_div\n",
    "        self.log(\"test_loss\", loss, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "batch_size = 8\n",
    "x = torch.randn(batch_size, 28 * 28 * 1)\n",
    "vae_pl = VAEpl(3e-4)\n",
    "x_reconstructed, mu, sigma = vae_pl(x)\n",
    "print(x_reconstructed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, dataset, digit, num_examples=1):\n",
    "    images = []\n",
    "    idx = 0\n",
    "    for x, y in dataset:\n",
    "        if y == idx:\n",
    "            images.append(x)\n",
    "            idx += 1\n",
    "        if idx == 10:\n",
    "            break\n",
    "\n",
    "    encodings_digit = []\n",
    "    for d in range(10):\n",
    "        with torch.no_grad():\n",
    "            mu, sigma = model.encode(images[d].view(1, 784))\n",
    "        encodings_digit.append((mu, sigma))\n",
    "\n",
    "    mu, sigma = encodings_digit[digit]\n",
    "    for example in range(num_examples):\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z = mu + sigma * epsilon\n",
    "        out = model.decode(z)\n",
    "        out = out.view(-1, 1, 28, 28)\n",
    "        save_image(out, f\"generated_{digit}_ex{example}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/anaconda3/lib/python3.11/site-packages/lightning_fabric/connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-4\n",
    "batch_size = 32\n",
    "num_workers = 20\n",
    "model = VAEpl(lr)\n",
    "module = MNISTDataModule(batch_size, num_workers)\n",
    "logger = TensorBoardLogger(\"my_checkpoint\", name=\"scheduler_autolr_vae_pl_model\")\n",
    "\n",
    "callbacks = [\n",
    "             pl.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n",
    "             pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_top_k=1, mode=\"min\", save_last=True),\n",
    "            ]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    precision=16,\n",
    "    strategy='ddp_notebook'\n",
    ")\n",
    "\n",
    "trainer.fit(model, module) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[W socket.cpp:436] [c10d] The server socket has failed to bind to [::]:46465 (errno: 98 - Address already in use).\n",
      "[W socket.cpp:436] [c10d] The server socket has failed to bind to 0.0.0.0:46465 (errno: 98 - Address already in use).\n",
      "[E socket.cpp:472] [c10d] The server socket has failed to listen on any local network address.\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 74, in _wrap\n    fn(i, *args)\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 170, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 795, in _test_impl\n    results = self._run(model, ckpt_path=ckpt_path)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 948, in _run\n    self.strategy.setup_environment()\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/ddp.py\", line 146, in setup_environment\n    self.setup_distributed()\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/ddp.py\", line 197, in setup_distributed\n    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/lightning_fabric/utilities/distributed.py\", line 290, in _init_dist_connection\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 74, in wrapper\n    func_return = func(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1141, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/torch/distributed/rendezvous.py\", line 241, in _env_rendezvous_handler\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/torch/distributed/rendezvous.py\", line 172, in _create_c10d_store\n    return TCPStore(\n           ^^^^^^^^^\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:46465 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:46465 (errno: 98 - Address already in use).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/maxim/Documents/TestProject/cnn autoencoder/VAE/code.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/cnn%20autoencoder/VAE/code.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39mtest(model, module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:755\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    754\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtesting \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    756\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[1;32m    757\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:141\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m process_context \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mstart_processes(\n\u001b[1;32m    134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapping_function,\n\u001b[1;32m    135\u001b[0m     args\u001b[39m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     join\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,  \u001b[39m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocs \u001b[39m=\u001b[39m process_context\u001b[39m.\u001b[39mprocesses\n\u001b[0;32m--> 141\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m process_context\u001b[39m.\u001b[39mjoin():\n\u001b[1;32m    142\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    144\u001b[0m worker_output \u001b[39m=\u001b[39m return_queue\u001b[39m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:163\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    161\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n\u001b[1;32m    162\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m original_trace\n\u001b[0;32m--> 163\u001b[0m \u001b[39mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[39m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 74, in _wrap\n    fn(i, *args)\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 170, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 795, in _test_impl\n    results = self._run(model, ckpt_path=ckpt_path)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 948, in _run\n    self.strategy.setup_environment()\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/ddp.py\", line 146, in setup_environment\n    self.setup_distributed()\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/ddp.py\", line 197, in setup_distributed\n    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/lightning_fabric/utilities/distributed.py\", line 290, in _init_dist_connection\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 74, in wrapper\n    func_return = func(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1141, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/torch/distributed/rendezvous.py\", line 241, in _env_rendezvous_handler\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/maxim/anaconda3/lib/python3.11/site-packages/torch/distributed/rendezvous.py\", line 172, in _create_c10d_store\n    return TCPStore(\n           ^^^^^^^^^\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:46465 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:46465 (errno: 98 - Address already in use).\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model, module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
