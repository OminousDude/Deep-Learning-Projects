{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "379efde3-f91e-4cb5-a404-42ea52e3a4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import MNIST, Flowers102, StanfordCars, CIFAR10\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b1d473bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_T = 1000\n",
    "device = \"cuda\"\n",
    "n_classes = 2\n",
    "n_feat = 128\n",
    "l_rate = 0.00008\n",
    "save_model = False\n",
    "save_dir = './imgs/'\n",
    "img_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "98911ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual convolutional block with two convolutinal layers and GELU activation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, is_res = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Check if input and output channels are same for resiual connectoin\n",
    "        self.same_channels = in_channels==out_channels\n",
    "        # Whether to perform residual connection\n",
    "        self.is_res = is_res\n",
    "\n",
    "        # First convolutoinal layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Second convolutoinal layer\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.is_res:\n",
    "            # Residual\n",
    "            x1 = self.conv1(x)\n",
    "            # Second conv layer\n",
    "            x2 = self.conv2(x1)\n",
    "\n",
    "            # Add residual connection based on channels\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2\n",
    "\n",
    "            return out\n",
    "        else:\n",
    "            # Non-residual\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4113e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetDown(nn.Module):\n",
    "    \"\"\"\n",
    "    A downscampling block for my cnn model.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cc0070cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetUp(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling block for my cnn model.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Concatenate the input with the skip connection of resnet\n",
    "        x = torch.cat((x, skip), 1)\n",
    "\n",
    "        # Run the concatanated tensor throgh the model\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8aa9a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedFC(nn.Module):\n",
    "    \"\"\"\n",
    "    A network for embedding features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the input tensor to a single dimensoin\n",
    "        x = x.view(-1, self.input_dim)\n",
    "\n",
    "        # pass the input through the embeding network\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "12918953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat = 128, n_classes = 10):\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Initial conv\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Downsampling blocks\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "\n",
    "        # Feacter vect extract for context and time embeddings\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(8), nn.GELU())\n",
    "\n",
    "        # Embeddings for context and time info\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n",
    "\n",
    "        # Upscaling blocks\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 8, 8),\n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Output conv\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t, context_mask):\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        # Downsampling\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "\n",
    "        # print(\"hi1\")\n",
    "\n",
    "        # Feature vector for context and time embedding\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # print(c)\n",
    "\n",
    "        # print('hi2')\n",
    "\n",
    "        # Convert context information to one-hot encoding and apply context mask\n",
    "        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n",
    "        \n",
    "        # print(c)\n",
    "\n",
    "        # print('hi3')\n",
    "        # print(c)\n",
    "\n",
    "        # print(c)\n",
    "        # print(c.shape)\n",
    "\n",
    "        context_mask = context_mask[:, None].repeat(1, self.n_classes) * -1 + 1\n",
    "        c = c * context_mask\n",
    "\n",
    "        # print('hi4')\n",
    "\n",
    "        # Generate context and time embeddings\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "\n",
    "\n",
    "        # print('hi5')\n",
    "\n",
    "        # Upsampling path with context and time embeddings\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1+ temb1, down2)\n",
    "        up3 = self.up2(cemb2*up2+ temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "\n",
    "        # print('hi6')\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "990b01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    \"\"\"\n",
    "    This function computes and returns pre-computed schedules for DDPM sampling and training.\n",
    "    \"\"\"\n",
    "\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    # Compute values for nois\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
    "    \n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,\n",
    "        \"oneover_sqrta\": oneover_sqrta,\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,\n",
    "        \"alphabar_t\": alphabar_t,\n",
    "        \"sqrtab\": sqrtab,\n",
    "        \"sqrtmab\": sqrtmab,\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a4eb6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "61c9b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvtImg(img, channels=3):\n",
    "    if channels == 3:\n",
    "        img = img\n",
    "    elif channels == 4:\n",
    "        img = img[0]\n",
    "    img = img - img.min()\n",
    "    img = (img / img.max())\n",
    "    return tf(img.numpy().astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6882d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(pl.LightningModule):\n",
    "    def __init__(self, nn_model=ContextUnet(in_channels=3, n_feat=n_feat, n_classes=n_classes),\n",
    "                 betas=(1e-4, 0.02), n_T=n_T, device=device, timesteps=n_T,\n",
    "                 device_=device, drop_prob=0.1, learning_rate=1e-4, num_classes=n_classes):\n",
    "        super(DDPM, self).__init__()\n",
    "\n",
    "        # Initialize the neural network model\n",
    "        self.nn_model = nn_model.to(device_)\n",
    "\n",
    "        # Store the num of class\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Register pre-computed schedules for diffusion process (noise gen)\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.step = 0\n",
    "        self.n_T = timesteps\n",
    "        self.drop_prob = drop_prob\n",
    "        self.loss_mse = nn.SmoothL1Loss()\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # Sample a random diffusion timestep\n",
    "        _ts = torch.randint(1, self.n_T+1, (x.shape[0],)).to(self.device)\n",
    "\n",
    "        # Generate noise for diffusion process\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "        # Compute the denoised image at the chosen timestep\n",
    "        x_t = (\n",
    "            self.sqrtab[_ts, None, None, None] * x\n",
    "            + self.sqrtmab[_ts, None, None, None] * noise\n",
    "        )\n",
    "\n",
    "        # Apply context mask with dropout\n",
    "        context_mask = torch.bernoulli(torch.zeros_like(c, dtype=torch.float) + self.drop_prob).to(self.device)\n",
    "\n",
    "        # Compute the loss between the prediced noise and the actual noise\n",
    "        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))\n",
    "\n",
    "    def sample(self, n_sample, size, device, guide_w = 1.0):\n",
    "        # Initialize the starting noise\n",
    "        x_i = torch.randn(n_sample, *size).to(device)\n",
    "\n",
    "        # Generate class labels for conditional image gen\n",
    "        c_i = torch.arange(0, self.num_classes).to(device)\n",
    "        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))\n",
    "\n",
    "        # Initialize the context mask for conditional generation\n",
    "        context_mask = torch.zeros_like(c_i).to(device)\n",
    "\n",
    "        # Duplicate the inputs for guide comparison\n",
    "        c_i = c_i.repeat(2)\n",
    "        context_mask = context_mask.repeat(2)\n",
    "        context_mask[n_sample:] = 1.\n",
    "\n",
    "        x_i_store = []\n",
    "\n",
    "        print()\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            print(f'sampling timestep {i}',end='\\r')\n",
    "            t_is = torch.tensor([i / self.n_T]).to(device)\n",
    "            t_is = t_is.repeat(n_sample,1,1,1)\n",
    "\n",
    "            x_i = x_i.repeat(2,1,1,1)\n",
    "            t_is = t_is.repeat(2,1,1,1)\n",
    "\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "\n",
    "            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n",
    "            eps1 = eps[:n_sample]\n",
    "            eps2 = eps[n_sample:]\n",
    "            eps = (1+guide_w)*eps1 - guide_w*eps2\n",
    "            x_i = x_i[:n_sample]\n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "\n",
    "            if i%20==0 or i==self.n_T:\n",
    "                x_i_store.append(x_i.detach().cpu().numpy())\n",
    "\n",
    "        x_i_store = np.array(x_i_store)\n",
    "        return cvtImg(x_i.detach().cpu(), 4), x_i_store\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), self.learning_rate)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, c = batch\n",
    "        loss_ema = None\n",
    "        \n",
    "        loss = self(x, c)\n",
    "\n",
    "        if loss_ema is None:\n",
    "            loss_ema = loss.item()\n",
    "        else:\n",
    "            loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
    "\n",
    "        if self.step % 5000 == 0 and self.step != 0:\n",
    "            print(f\"Step {self.step:03d} | Loss: {loss.item()}\")\n",
    "            \n",
    "            dict = self.state_dict()\n",
    "\n",
    "            dict[\"pytorch-lightning_version\"] = '2.1.0'\n",
    "            dict[\"global_step\"] = self.step\n",
    "            dict[\"epoch\"] = self.current_epoch\n",
    "            dict[\"state_dict\"] = self.state_dict()\n",
    "\n",
    "            now = datetime.now()\n",
    "\n",
    "            dt_string = now.strftime(\"%d|%m|%Y %H:%M:%S\")\n",
    "            \n",
    "            torch.save(dict, \"saves/diffusion_model_step: \" + self.step.__str__() + \"|time: \" + dt_string + \".ckpt\")\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, c = batch\n",
    "\n",
    "        n_sample = 4*n_classes\n",
    "        x_gen, x_gen_store = self.sample(n_sample, (3, img_size, img_size), device, guide_w=1.0)\n",
    "\n",
    "        x_gen = x_gen.permute(1,2,0)\n",
    "        \n",
    "        # remove *-1\n",
    "\n",
    "        grid = make_grid(transforms.ToTensor()(transforms.ToPILImage()(x_gen)), nrow=10)\n",
    "        save_image(grid, save_dir + f\"image_{self.step}.png\")\n",
    "        print('saved image at ' + save_dir + f\"image{self.step}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8068c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations, attr, img_dir, transform=None):\n",
    "        self.img_labels = annotations\n",
    "        self.attr = attr\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx])\n",
    "        \n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        val = \"\"\n",
    "        for i in range(self.attr[idx].__len__()):\n",
    "            val = val + self.attr[idx][i].__str__()\n",
    "\n",
    "        return image, int(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ad607034",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "all_attr = pd.read_csv(\"/home/maxim/Downloads/archive (2)/list_attr_celeba.csv\", index_col=\"image_id\")\n",
    "all_attr[all_attr.columns] = (all_attr[all_attr.columns] + 1) / 2\n",
    "\n",
    "id = pd.read_csv(\"/home/maxim/Downloads/archive (2)/list_attr_celeba.csv\").pop(\"image_id\")\n",
    "\n",
    "attr_list = [\"Male\"]\n",
    "\n",
    "data = CustomImageDataset(id, all_attr[\n",
    "                                        attr_list\n",
    "                                      ].astype(int).values,\n",
    "                                               \"/home/maxim/Downloads/archive (2)/img_align_celeba/img_align_celeba\",\n",
    "                                                 transform)\n",
    "\n",
    "data = torch.utils.data.ConcatDataset([data])\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4982cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tf = transforms.Compose([\n",
    "#                                transforms.Grayscale(3),\n",
    "#                                transforms.Resize(28),\n",
    "#                                transforms.ToTensor(),\n",
    "#                                transforms.Normalize((0.5,), (0.5,)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "97090f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c9d75136",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "63ecf33b-edf2-41cf-adcd-9475d492522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type         | Params\n",
      "------------------------------------------\n",
      "0 | nn_model | ContextUnet  | 7.6 M \n",
      "1 | loss_mse | SmoothL1Loss | 0     \n",
      "------------------------------------------\n",
      "7.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.6 M     Total params\n",
      "30.281    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdbe5f81b034ba08f342b830beacf91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# dataset = CIFAR10(\"./data\", download=True, transform=tf)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=20)\n",
    "\n",
    "ddpm = DDPM(nn_model=ContextUnet(in_channels=3, n_feat=n_feat, n_classes=2),\n",
    "             betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1, learning_rate=l_rate, num_classes=n_classes)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=device, max_epochs=500, precision='16-mixed')\n",
    "\n",
    "# ddpm = DDPM.load_from_checkpoint('saves/diffusion_model_step: 5000|time: 14|12|2023 10:08:22.ckpt')\n",
    "\n",
    "trainer.fit(ddpm, train_dataloaders=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09092e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/maxim/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c61f6d8127e467ca294a8007c1d306e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saved image at ./imgs/image24192.png\n",
      "\n",
      "saved image at ./imgs/image24192.png\n",
      "\n",
      "saved image at ./imgs/image24192.png\n",
      "\n",
      "saved image at ./imgs/image24192.png\n",
      "\n",
      "saved image at ./imgs/image24192.png\n",
      "\n",
      "sampling timestep 8770\r"
     ]
    }
   ],
   "source": [
    "trainer.validate(ddpm, dataloaders=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
