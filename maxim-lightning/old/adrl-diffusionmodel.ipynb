{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Functions for forward diffusion"]},{"cell_type":"markdown","metadata":{},"source":["# Diffusion Model"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T11:47:58.642093Z","iopub.status.busy":"2022-11-10T11:47:58.641051Z","iopub.status.idle":"2022-11-10T11:48:01.455045Z","shell.execute_reply":"2022-11-10T11:48:01.453892Z","shell.execute_reply.started":"2022-11-10T11:47:58.641952Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from glob import glob\n","import matplotlib.pyplot as plt\n","from skimage import io\n","import numpy as np\n","\n","IMG_SIZE = 64\n","BATCH_SIZE = 32\n","\n","def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n","    return torch.linspace(start, end, timesteps)\n","\n","def get_index_from_list(vals, t, x_shape):\n","    \"\"\" \n","    Returns a specific index t of a passed list of values vals\n","    while considering the batch dimension.\n","    \"\"\"\n","    batch_size = t.shape[0]\n","    out = vals.gather(-1, t.cpu())\n","    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n","\n","def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n","    \"\"\" \n","    Takes an image and a timestep as input and \n","    returns the noisy version of it\n","    \"\"\"\n","    noise = torch.randn_like(x_0)\n","    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n","    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n","        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n","    )\n","    # mean + variance\n","    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Using 300 time steps"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T11:48:01.45798Z","iopub.status.busy":"2022-11-10T11:48:01.457249Z","iopub.status.idle":"2022-11-10T11:48:01.487959Z","shell.execute_reply":"2022-11-10T11:48:01.486753Z","shell.execute_reply.started":"2022-11-10T11:48:01.457929Z"},"trusted":true},"outputs":[],"source":["# Define beta schedule\n","T = 300\n","betas = linear_beta_schedule(timesteps=T)\n","\n","# Pre-calculate different terms for closed form\n","alphas = 1. - betas\n","alphas_cumprod = torch.cumprod(alphas, axis=0)\n","alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n","sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n","sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n","sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n","posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T11:48:01.490431Z","iopub.status.busy":"2022-11-10T11:48:01.489724Z","iopub.status.idle":"2022-11-10T11:48:01.504199Z","shell.execute_reply":"2022-11-10T11:48:01.50295Z","shell.execute_reply.started":"2022-11-10T11:48:01.490384Z"},"trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","  def __init__(self, root_dir, transform=transforms.Compose([transforms.ToTensor(),\n","                                                             transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","                                                             transforms.RandomHorizontalFlip(),\n","                                                             transforms.Lambda(lambda t: (t * 2) - 1)\n","                                                             ])):\n","    self.root_dir = root_dir\n","    self.transform = transform\n","    self.img_list = glob(self.root_dir+\"/*\")\n","\n","  def __len__(self):\n","    return len(self.img_list)\n","\n","  def __getitem__(self, idx):\n","    image = io.imread(self.img_list[idx])\n","\n","    if self.transform:\n","      image = self.transform(image)\n","\n","    return image\n","\n","def show_tensor_image(image):\n","    reverse_transforms = transforms.Compose([\n","        transforms.Lambda(lambda t: (t+1) / 2),\n","        transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n","        transforms.Lambda(lambda t: t * 255),\n","        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n","        transforms.ToPILImage(),\n","    ])\n","    \n","    if len(image.shape) == 4:\n","        image = image[0, :, :, :]\n","    plt.imshow(reverse_transforms(image))\n","    plt.axis(\"off\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T11:48:01.508144Z","iopub.status.busy":"2022-11-10T11:48:01.507361Z","iopub.status.idle":"2022-11-10T11:48:01.519221Z","shell.execute_reply":"2022-11-10T11:48:01.517909Z","shell.execute_reply.started":"2022-11-10T11:48:01.508096Z"},"trusted":true},"outputs":[],"source":["# # Testing the forward diffusion step\n","# image = next(iter(train_loader))[0]\n","\n","# plt.figure(figsize = (15, 15))\n","# plt.axis(\"off\")\n","# num_images = 10\n","# stepsize = int(T / num_images)\n","\n","# for idx in range(0, T, stepsize):\n","#     t = torch.tensor([idx]).type(torch.int64)\n","#     plt.subplot(1, num_images+1, (idx // stepsize) + 1)\n","#     show_tensor_image(image)\n","#     image, noise = forward_diffusion_sample(image, t)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["data = CustomDataset('/home/maxim/Documents/TestProject/maxim-lightning/archive (4)/bitmojis/')\n","dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T11:48:01.523308Z","iopub.status.busy":"2022-11-10T11:48:01.522294Z","iopub.status.idle":"2022-11-10T11:48:01.545842Z","shell.execute_reply":"2022-11-10T11:48:01.544459Z","shell.execute_reply.started":"2022-11-10T11:48:01.523257Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","import math\n","\n","import pytorch_lightning as pl\n","\n","\n","class Block(nn.Module):\n","    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n","        super().__init__()\n","        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n","        if up:\n","            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n","            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n","        else:\n","            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n","            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n","        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n","        self.bnorm1 = nn.BatchNorm2d(out_ch)\n","        self.bnorm2 = nn.BatchNorm2d(out_ch)\n","        self.relu  = nn.ReLU()\n","        \n","    def forward(self, x, t, ):\n","        # First Conv\n","        h = self.bnorm1(self.relu(self.conv1(x)))\n","        # Time embedding\n","        time_emb = self.relu(self.time_mlp(t))\n","        # Extend last 2 dimensions\n","        time_emb = time_emb[(..., ) + (None, ) * 2]\n","        # Add time channel\n","        h = h + time_emb\n","        # Second Conv\n","        h = self.bnorm2(self.relu(self.conv2(h)))\n","        # Down or Upsample\n","        return self.transform(h)\n","\n","class SinusoidalPositionEmbeddings(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, time):\n","        device = time.device\n","        half_dim = self.dim // 2\n","        embeddings = math.log(10000) / (half_dim - 1)\n","        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n","        embeddings = time[:, None] * embeddings[None, :]\n","        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n","        # TODO: Double check the ordering here\n","        return embeddings\n","\n","\n","class SimpleUnet(pl.LightningModule):\n","    def __init__(self):\n","        super().__init__()\n","        image_channels = 3\n","        down_channels = (64, 128, 256, 512, 1024)\n","        up_channels = (1024, 512, 256, 128, 64)\n","        out_dim = 1 \n","        time_emb_dim = 32\n","\n","        # Time embedding\n","        self.time_mlp = nn.Sequential(\n","                SinusoidalPositionEmbeddings(time_emb_dim),\n","                nn.Linear(time_emb_dim, time_emb_dim),\n","                nn.ReLU()\n","            )\n","        \n","        # Initial projection\n","        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n","\n","        # Downsample\n","        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\\n","                                    time_emb_dim) \\\n","                    for i in range(len(down_channels)-1)])\n","        # Upsample\n","        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\\n","                                        time_emb_dim, up=True) \\\n","                    for i in range(len(up_channels)-1)])\n","\n","        self.output = nn.Conv2d(up_channels[-1], 3, out_dim)\n","\n","    def forward(self, x, timestep):\n","        # Embedd time\n","        t = self.time_mlp(timestep)\n","        # Initial conv\n","        x = self.conv0(x)\n","        # Unet\n","        residual_inputs = []\n","        for down in self.downs:\n","            x = down(x, t)\n","            residual_inputs.append(x)\n","        for up in self.ups:\n","            residual_x = residual_inputs.pop()\n","            # Add residual x as additional channels\n","            x = torch.cat((x, residual_x), dim=1)           \n","            x = up(x, t)\n","        return self.output(x)\n","    \n","    def prepare_data(self):\n","        transform = transforms.Compose(\n","            [\n","                transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","                transforms.ToTensor(),\n","            ]\n","        )\n","\n","        end_train_idx = int(len(dataloader.dataset) - len(dataloader.dataset) / 5)\n","        end_val_idx = int(len(dataloader.dataset) - len(dataloader.dataset) / 7)\n","        end_test_idx = len(dataloader.dataset)\n","\n","        self.train_dataset = Subset(dataloader.dataset, range(0, end_train_idx))\n","        self.val_dataset = Subset(dataloader.dataset, range(end_train_idx + 1, end_val_idx))\n","        self.test_dataset = Subset(dataloader.dataset, range(end_val_idx + 1, end_test_idx))\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=20\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.val_dataset, batch_size=1, num_workers=20\n","        )\n","\n","    def test_dataloader(self):\n","        return DataLoader(\n","            self.test_dataset, batch_size=1, num_workers=20\n","        )\n","    \n","    def configure_optimizers(self):\n","         return torch.optim.Adam(self.parameters(), lr=0.0007)\n","\n","    def training_step(self, batch, batch_idx):\n","        x0 = batch[0]\n","        n = len(x0)\n","        \n","        t = torch.randint(0, T, (n,), device=\"cuda\")\n","        \n","        loss = get_loss(self, batch[0], t)\n","\n","        self.step += 1\n","\n","        if self.step % 500 == 0:\n","            sample_plot_image(self)\n","            print(f\"Step {self.step:03d} | Loss: {loss.item()}\")\n","            torch.save(self.state_dict(), \"diffusion_model_celeba.pth\")\n","\n","        logs = {\"loss\": loss}\n","        return {\"loss\": loss, \"log\": logs}\n","    \n","    def validation_step(self, batch, batch_idx):\n","        x = batch\n","\n","        t = torch.randint(T - 5, T, (1,), device=\"cuda\").long()\n","\n","        output = self(x, t)\n","\n","        loss = get_loss(self, batch[0], t)\n","\n","        reverse_transforms = v2.Compose([\n","            transforms.Lambda(lambda t: (t + 1) / 2),\n","            transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n","            transforms.Lambda(lambda t: t * 255.),\n","            transforms.Lambda(lambda t: np.absolute(t.numpy()).astype(np.uint8)),\n","            transforms.ToTensor()\n","        ])\n","\n","        output_img = output[0].cpu()\n","\n","        self.val_outputs.append(reverse_transforms(output_img))\n","\n","        self.log(\"val loss\", loss)\n","\n","        logs = {\"loss\": loss}\n","        return {\"loss\": loss, \"log\": logs}\n","    \n","    def on_validation_epoch_end(self):\n","        grid = vutils.make_grid(self.val_outputs)\n","        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)\n","\n","        self.val_outputs.clear()"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T11:48:01.550287Z","iopub.status.busy":"2022-11-10T11:48:01.547416Z","iopub.status.idle":"2022-11-10T11:48:01.560467Z","shell.execute_reply":"2022-11-10T11:48:01.559329Z","shell.execute_reply.started":"2022-11-10T11:48:01.550238Z"},"trusted":true},"outputs":[],"source":["def get_loss(model, x_0, t):\n","    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n","    noise_pred = model(x_noisy, t)\n","    print((torch.isnan(noise_pred.view(-1)).sum().item()==0).__str__())\n","    return F.l1_loss(noise, noise_pred) # can use l2 loss"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:14:04.071624Z","iopub.status.busy":"2022-11-10T18:14:04.07087Z","iopub.status.idle":"2022-11-10T18:14:04.082413Z","shell.execute_reply":"2022-11-10T18:14:04.081242Z","shell.execute_reply.started":"2022-11-10T18:14:04.071587Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def sample_timestep(x, t, model):\n","    \"\"\"\n","    Calls the model to predict the noise in the image and returns \n","    the denoised image. \n","    Applies noise to this image, if we are not in the last step yet.\n","    \"\"\"\n","    betas_t = get_index_from_list(betas, t, x.shape)\n","    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n","        sqrt_one_minus_alphas_cumprod, t, x.shape\n","    )\n","    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n","    \n","    # Call model (current image - noise prediction)\n","    model_mean = sqrt_recip_alphas_t * (\n","        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n","    )\n","    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n","    \n","    if t == 0:\n","        return model_mean\n","    else:\n","        noise = torch.randn_like(x)\n","        return model_mean + torch.sqrt(posterior_variance_t) * noise \n","\n","@torch.no_grad()\n","def sample_plot_image(model):\n","    # Sample noise\n","    img_size = IMG_SIZE\n","    img = torch.randn((1, 3, img_size, img_size), device=device)\n","    plt.figure(figsize=(20,20))\n","    plt.axis('off')\n","    num_images = 10\n","    stepsize = int(T/num_images)\n","\n","    for i in range(0,T)[::-1]:\n","        t = torch.full((1,), i, device=device, dtype=torch.long)\n","        img = sample_timestep(img, t, model)\n","        if i % stepsize == 0:\n","            plt.subplot(1, num_images, i//stepsize+1)\n","            show_tensor_image(img.detach().cpu())\n","    plt.show()            "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["celeba_dataset = CustomDataset(root_dir = \"/home/maxim/Documents/TestProject/maxim-lightning/archive (4)/bitmojis\")\n","train_loader = DataLoader(dataset=celeba_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last = True)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["SimpleUnet(\n","  (time_mlp): Sequential(\n","    (0): SinusoidalPositionEmbeddings()\n","    (1): Linear(in_features=32, out_features=32, bias=True)\n","    (2): ReLU()\n","  )\n","  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (downs): ModuleList(\n","    (0): Block(\n","      (time_mlp): Linear(in_features=32, out_features=128, bias=True)\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (transform): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (1): Block(\n","      (time_mlp): Linear(in_features=32, out_features=256, bias=True)\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (transform): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (2): Block(\n","      (time_mlp): Linear(in_features=32, out_features=512, bias=True)\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (transform): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (3): Block(\n","      (time_mlp): Linear(in_features=32, out_features=1024, bias=True)\n","      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (transform): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bnorm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bnorm2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","  )\n","  (ups): ModuleList(\n","    (0): Block(\n","      (time_mlp): Linear(in_features=32, out_features=512, bias=True)\n","      (conv1): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (transform): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (1): Block(\n","      (time_mlp): Linear(in_features=32, out_features=256, bias=True)\n","      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (transform): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (2): Block(\n","      (time_mlp): Linear(in_features=32, out_features=128, bias=True)\n","      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (transform): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (3): Block(\n","      (time_mlp): Linear(in_features=32, out_features=64, bias=True)\n","      (conv1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (transform): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","  )\n","  (output): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["model = SimpleUnet()\n","model"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["from pytorch_lightning.callbacks import ModelCheckpoint"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["checkpoint_callback = ModelCheckpoint(dirpath='/home/maxim/Documents/TestProject/maxim-lightning/checkpoints/', every_n_epochs = 5)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Using 16bit Automatic Mixed Precision (AMP)\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/tmp/ipykernel_5643/2420499670.py:3: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n","  with torch.autograd.detect_anomaly():\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name     | Type       | Params\n","----------------------------------------\n","0 | time_mlp | Sequential | 1.1 K \n","1 | conv0    | Conv2d     | 1.8 K \n","2 | downs    | ModuleList | 41.2 M\n","3 | ups      | ModuleList | 21.3 M\n","4 | output   | Conv2d     | 195   \n","----------------------------------------\n","62.4 M    Trainable params\n","0         Non-trainable params\n","62.4 M    Total params\n","249.756   Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ccc895f5ca64457fafe001c7fe3b76cd","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/home/maxim/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"ename":"ValueError","evalue":"expected 4D input (got 3D input)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, precision\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m16-mixed\u001b[39m\u001b[39m'\u001b[39m, accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m, callbacks\u001b[39m=\u001b[39m[checkpoint_callback])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mdetect_anomaly():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     trainer\u001b[39m.\u001b[39mfit(model)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     trainer\u001b[39m.\u001b[39mvalidate(model)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    544\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m call\u001b[39m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    546\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    547\u001b[0m )\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    576\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    577\u001b[0m     ckpt_path,\n\u001b[1;32m    578\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    580\u001b[0m )\n\u001b[0;32m--> 581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(model, ckpt_path\u001b[39m=\u001b[39mckpt_path)\n\u001b[1;32m    583\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    987\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_stage()\n\u001b[1;32m    992\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    995\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1034\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1033\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1034\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1035\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1036\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1063\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1060\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1062\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m val_loop\u001b[39m.\u001b[39mrun()\n\u001b[1;32m   1065\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1067\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39mstep_args)\n\u001b[1;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mvalidation_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","\u001b[1;32m/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(T \u001b[39m-\u001b[39m \u001b[39m5\u001b[39m, T, (\u001b[39m1\u001b[39m,), device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mlong()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, t)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m loss \u001b[39m=\u001b[39m get_loss(\u001b[39mself\u001b[39m, batch[\u001b[39m0\u001b[39m], t)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m reverse_transforms \u001b[39m=\u001b[39m v2\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m     transforms\u001b[39m.\u001b[39mLambda(\u001b[39mlambda\u001b[39;00m t: (t \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m),\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     transforms\u001b[39m.\u001b[39mLambda(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m ])\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m output_img \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\n","\u001b[1;32m/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_loss\u001b[39m(model, x_0, t):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     x_noisy, noise \u001b[39m=\u001b[39m forward_diffusion_sample(x_0, t, device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     noise_pred \u001b[39m=\u001b[39m model(x_noisy, t)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m((torch\u001b[39m.\u001b[39misnan(noise_pred\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__str__\u001b[39m())\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39ml1_loss(noise, noise_pred)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32m/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb Cell 17\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m residual_inputs \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mfor\u001b[39;00m down \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdowns:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     x \u001b[39m=\u001b[39m down(x, t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     residual_inputs\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mfor\u001b[39;00m up \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mups:\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32m/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, t, ):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# First Conv\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbnorm1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# Time embedding\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/maxim/Documents/TestProject/maxim-lightning/adrl-diffusionmodel.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     time_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_mlp(t))\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_input_dim(\u001b[39minput\u001b[39m)\n\u001b[1;32m    140\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:416\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    415\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m--> 416\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected 4D input (got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: expected 4D input (got 3D input)"]}],"source":["trainer = pl.Trainer(max_epochs=2, precision='16-mixed', accelerator=\"cuda\", callbacks=[checkpoint_callback])\n","\n","with torch.autograd.detect_anomaly():\n","    trainer.fit(model)\n","    trainer.validate(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T11:48:01.580999Z","iopub.status.busy":"2022-11-10T11:48:01.579513Z","iopub.status.idle":"2022-11-10T13:30:37.188656Z","shell.execute_reply":"2022-11-10T13:30:37.187501Z","shell.execute_reply.started":"2022-11-10T11:48:01.580952Z"},"trusted":true},"outputs":[],"source":["# from torch.optim import Adam\n","# from tqdm import tqdm\n","\n","# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# model = SimpleUnet()\n","# model.to(device)\n","# optimizer = Adam(model.parameters(), lr=0.001)\n","# epochs = 4 # Try more!\n","\n","# print(\"# of steps:\", len(train_loader))\n","# for epoch in range(epochs):\n","#     for step, batch in tqdm(enumerate(train_loader), position=0, leave=True):\n","#         optimizer.zero_grad()\n","#         t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n","#         batch = batch.to(device)\n","#         loss = get_loss(model, batch, t)\n","#         loss.backward()\n","#         optimizer.step()\n","#         if step % 3000 == 0:\n","#             sample_plot_image(model)\n","#             print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()}\")\n","#             torch.save(model.state_dict(), \"diffusion_model_celeba.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:14:09.230148Z","iopub.status.busy":"2022-11-10T18:14:09.22965Z"},"trusted":true},"outputs":[],"source":["from torch.optim import Adam\n","from tqdm import tqdm\n","\n","# BATCH_SIZE = 32\n","bitmoji_dataset = CustomDataset(root_dir = \"/kaggle/input/bitmojis/bitmojis/\")\n","train_loader = DataLoader(dataset=bitmoji_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last = True)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# bitmoji_model = SimpleUnet()\n","bitmoji_model.to(device)\n","optimizer = Adam(bitmoji_model.parameters(), lr=0.001)\n","epochs = 4 # Try more!\n","\n","print(\"# of batches:\", len(train_loader))\n","for epoch in range(epochs):\n","    for step, batch in tqdm(enumerate(train_loader), position=0, leave=True):\n","        optimizer.zero_grad()\n","        t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n","        batch = batch.to(device)\n","        loss = get_loss(bitmoji_model, batch, t)\n","        loss.backward()\n","        optimizer.step()\n","        if step % 1500 == 0:\n","            sample_plot_image(bitmoji_model)\n","            print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()}\")\n","            torch.save(bitmoji_model.state_dict(), \"diffusion_model_bitmoji.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T15:00:16.322525Z","iopub.status.busy":"2022-11-10T15:00:16.322099Z","iopub.status.idle":"2022-11-10T15:00:18.712854Z","shell.execute_reply":"2022-11-10T15:00:18.71146Z","shell.execute_reply.started":"2022-11-10T15:00:16.32249Z"},"trusted":true},"outputs":[],"source":["# Creating directory of 1000 original images\n","from PIL import Image\n","import shutil\n","import os\n","\n","path1 = \"/kaggle/output/og_imgs/\"\n","path2 = \"/kaggle/output/gen_imgs/\"\n","\n","path = \"/kaggle/input/bitmojis/bitmojis/*\"\n","img_list = glob(path)\n","\n","os.makedirs(path1, exist_ok = True)\n","os.makedirs(path2, exist_ok = True)\n","\n","for i in range(1000):\n","    shutil.copy(img_list[i], path1+f'{i}.jpeg')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T15:11:57.482801Z","iopub.status.busy":"2022-11-10T15:11:57.482394Z","iopub.status.idle":"2022-11-10T16:25:29.955365Z","shell.execute_reply":"2022-11-10T16:25:29.953921Z","shell.execute_reply.started":"2022-11-10T15:11:57.482767Z"},"trusted":true},"outputs":[],"source":["gen_imgs = np.zeros((1000, 64, 64, 3))\n","start = 0\n","batch_size = 20\n","\n","@torch.no_grad()\n","def sample_plot_image(model):\n","    # Sample noise\n","    img_size = IMG_SIZE\n","    img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n","    plt.figure(figsize=(20,20))\n","    plt.axis('off')\n","\n","    for i in range(0,T)[::-1]:\n","        t = torch.full((1,), i, device=device, dtype=torch.long)\n","        img = sample_timestep(img, t, model)\n","        if i  == 0:\n","            show_tensor_image(img.detach().cpu())\n","    plt.show()            \n","\n","for j in tqdm(range(0, 1000)):\n","    img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n","    for i in range(0,T)[::-1]:\n","        t = torch.full((1,), i, device=device, dtype=torch.long)\n","        img = sample_timestep(img, t, model)\n","        if i  == 0:\n","#             show_tensor_image(img.detach().cpu())\n","#             plt.show()\n","            gen_imgs[j] = img[0].permute(1, 2, 0).detach().cpu().numpy()\n","\n","for i in range(1000):\n","    Image.fromarray((gen_imgs[i]*255).astype(np.uint8)).save(path2 + f\"{i}.jpeg\")\n","\n","!pip install pytorch-fid\n","!python3 -m pytorch_fid ../output/og_imgs/ ../output/gen_imgs/ --device cpu"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T17:55:35.267525Z","iopub.status.busy":"2022-11-10T17:55:35.267054Z","iopub.status.idle":"2022-11-10T18:07:14.867182Z","shell.execute_reply":"2022-11-10T18:07:14.865678Z","shell.execute_reply.started":"2022-11-10T17:55:35.267481Z"},"trusted":true},"outputs":[],"source":["# Creating directory of 1000 original images\n","from PIL import Image\n","import shutil\n","import os\n","\n","path1 = \"/kaggle/celeba_output/og_imgs/\"\n","path2 = \"/kaggle/celeba_output/gen_imgs/\"\n","\n","path = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/*\"\n","img_list = glob(path)\n","\n","os.makedirs(path1, exist_ok = True)\n","os.makedirs(path2, exist_ok = True)\n","\n","for i in range(1000):\n","    shutil.copy(img_list[i], path1+f'{i}.jpeg')\n","    \n","gen_imgs = np.zeros((1000, 64, 64, 3))\n","start = 0\n","batch_size = 20\n","\n","@torch.no_grad()\n","def sample_plot_image(model):\n","    # Sample noise\n","    img_size = IMG_SIZE\n","    img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n","    plt.figure(figsize=(20,20))\n","    plt.axis('off')\n","\n","    for i in range(0,T)[::-1]:\n","        t = torch.full((1,), i, device=device, dtype=torch.long)\n","        img = sample_timestep(img, t, model)\n","        if i  == 0:\n","            show_tensor_image(img.detach().cpu())\n","    plt.show()            \n","\n","for j in tqdm(range(0, 1000)):\n","    img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n","    for i in range(0,T)[::-1]:\n","        t = torch.full((1,), i, device=device, dtype=torch.long)\n","        img = sample_timestep(img, t, bitmoji_model)\n","        if i  == 0:\n","            gen_imgs[j] = img[0].permute(1, 2, 0).detach().cpu().numpy()\n","\n","for i in range(1000):\n","    Image.fromarray((gen_imgs[i]*255).astype(np.uint8)).save(path2 + f\"{i}.jpeg\")\n","\n","!pip install pytorch-fid\n","!python3 -m pytorch_fid ../celeba_output/og_imgs/ ../celeba_output/gen_imgs/ --device cpu"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:10:57.836125Z","iopub.status.busy":"2022-11-10T18:10:57.834935Z","iopub.status.idle":"2022-11-10T18:11:40.920244Z","shell.execute_reply":"2022-11-10T18:11:40.919241Z","shell.execute_reply.started":"2022-11-10T18:10:57.836068Z"},"trusted":true},"outputs":[],"source":["img_size = IMG_SIZE\n","num_images = 10\n","stepsize = int(T/num_images)\n","\n","for k in range(10):\n","    img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n","    plt.figure(figsize=(20,20))\n","    plt.axis('off')\n","    for i in range(0,T)[::-1]:\n","        t = torch.full((1,), i, device=device, dtype=torch.long)\n","        img = sample_timestep(img, t, model)\n","        if i % stepsize == 0:\n","            plt.subplot(k+1, num_images, i//stepsize+1)\n","            show_tensor_image(img.detach().cpu())\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:11:59.899273Z","iopub.status.busy":"2022-11-10T18:11:59.89881Z","iopub.status.idle":"2022-11-10T18:12:43.233439Z","shell.execute_reply":"2022-11-10T18:12:43.232378Z","shell.execute_reply.started":"2022-11-10T18:11:59.899235Z"},"trusted":true},"outputs":[],"source":["img_size = IMG_SIZE\n","num_images = 10\n","stepsize = int(T/num_images)\n","\n","for k in range(10):\n","    img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n","    plt.figure(figsize=(20,20))\n","    plt.axis('off')\n","    for i in range(0,T)[::-1]:\n","        t = torch.full((1,), i, device=device, dtype=torch.long)\n","        img = sample_timestep(img, t, bitmoji_model)\n","        if i % stepsize == 0:\n","            plt.subplot(k+1, num_images, i//stepsize+1)\n","            show_tensor_image(img.detach().cpu())\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T19:41:33.255345Z","iopub.status.busy":"2022-11-10T19:41:33.254927Z","iopub.status.idle":"2022-11-10T19:47:51.039371Z","shell.execute_reply":"2022-11-10T19:47:51.036005Z","shell.execute_reply.started":"2022-11-10T19:41:33.255308Z"},"trusted":true},"outputs":[],"source":["img_size = IMG_SIZE\n","num_images = 10\n","stepsize = int(T/num_images)\n","\n","plt.figure(figsize=(20,20))\n","plt.axis('off')\n","for k in range(10):\n","    for j in range(10):\n","        img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n","        for i in range(0,T)[::-1]:\n","            t = torch.full((1,), i, device=device, dtype=torch.long)\n","            img = sample_timestep(img, t, bitmoji_model)\n","            if i == 0:\n","                plt.subplot(10, 10, 10*k + j + 1)\n","                show_tensor_image(img.detach().cpu())\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T19:48:25.313872Z","iopub.status.busy":"2022-11-10T19:48:25.313448Z","iopub.status.idle":"2022-11-10T19:54:41.933324Z","shell.execute_reply":"2022-11-10T19:54:41.931955Z","shell.execute_reply.started":"2022-11-10T19:48:25.313837Z"},"trusted":true},"outputs":[],"source":["img_size = IMG_SIZE\n","num_images = 10\n","stepsize = int(T/num_images)\n","\n","plt.figure(figsize=(20,20))\n","plt.axis('off')\n","for k in range(10):\n","    for j in range(10):\n","        img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n","        for i in range(0,T)[::-1]:\n","            t = torch.full((1,), i, device=device, dtype=torch.long)\n","            img = sample_timestep(img, t, model)\n","            if i == 0:\n","                plt.subplot(10, 10, 10*k + j + 1)\n","                show_tensor_image(img.detach().cpu())\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"tpuV38","dataSources":[{"datasetId":20173,"sourceId":26126,"sourceType":"datasetVersion"},{"datasetId":29561,"sourceId":37705,"sourceType":"datasetVersion"},{"datasetId":1071415,"sourceId":1803299,"sourceType":"datasetVersion"}],"dockerImageVersionId":30299,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
